Lab 1 â€“ Functions & What They Do (Very Short)

step_env(env,action)
Wrapper: handles both old Gym (4-return) and Gymnasium (5-return) env.step() formats.

make_frozenlake(det=True,size=4)
Create FrozenLake-v1 env â†’ deterministic (det=True) or stochastic (det=False).

explore_env(env)
Reset env and print initial state, action space, observation space.

run_random_episode(env,render=False,max_steps=100)
Run one episode with random actions; print each step; return total reward.

run_random_episodes(env,num_episodes=100,max_steps=100)
Run many random episodes; print total reward per episode; return list of rewards.

visualize_path(path,size=4)
Show a 4Ã—4 grid with step numbers marking the visited states.

random_path_episode(env,size=4,max_steps=100)
Run random episode, record visited states â†’ call visualize_path to draw path.

compare_det_vs_stoch(num_episodes=20,max_steps=100)
Run random episodes on deterministic vs stochastic FrozenLake; return both reward lists.

plot_random_policy_rewards(env,num_episodes=10,max_steps=100)
Run random policy, collect episode rewards, plot reward vs episode.

run_manual_policy(env,actions,num_episodes=5,max_steps=None)
Execute a fixed action sequence (manual policy) for several episodes; print transitions & reward.

explore_other_env(env_name='Taxi-v3',num_episodes=5,max_steps=200)
Try other envs (Taxi, Cliff, etc.) with random actions; print reward per episode.






Lab 2 â€“ Functions & Purpose (Very Short Cheatsheet)

init_gridworld(rows,cols,wall,goal,danger)
â†’ Create GridWorld MDP description (states, actions, wall, goal, danger).

reward(state,goal,danger)
â†’ Reward function: +1 goal, -1 danger, -0.04 otherwise.

next_state(state,action,rows,cols,wall)
â†’ Deterministic next state (handles borders + wall).

transition_probabilities(state,action,mdp,p_intended,p_left,p_right)
â†’ Stochastic transition model (80â€“10â€“10 by default, or custom).

print_transitions(state,action,mdp,...)
â†’ Print P(sâ€²|s,a) and reward for all next states (nice for exam explanation).

set_gamma(gamma)
â†’ Tiny helper to â€œset/useâ€ Î³ (mainly conceptual placeholder).

value_iteration(mdp,gamma,theta,...)
â†’ Run value iteration â†’ returns optimal V(s) and greedy policy(s) for GridWorld.

print_value_grid(V,mdp)
â†’ Pretty-print V(s) as a grid (with wall shown).

print_policy_grid(policy,mdp)
â†’ Pretty-print optimal policy as arrows/labels (GOAL, DANG, WALL).





Lab 2 â€“ Functions & Purpose (Very Short Cheatsheet)

init_gridworld(rows,cols,wall,goal,danger)
â†’ Create GridWorld MDP description (states, actions, wall, goal, danger).

reward(state,goal,danger)
â†’ Reward function: +1 goal, -1 danger, -0.04 otherwise.

next_state(state,action,rows,cols,wall)
â†’ Deterministic next state (handles borders + wall).

transition_probabilities(state,action,mdp,p_intended,p_left,p_right)
â†’ Stochastic transition model (80â€“10â€“10 by default, or custom).

print_transitions(state,action,mdp,...)
â†’ Print P(sâ€²|s,a) and reward for all next states (nice for exam explanation).

set_gamma(gamma)
â†’ Tiny helper to â€œset/useâ€ Î³ (mainly conceptual placeholder).

value_iteration(mdp,gamma,theta,...)
â†’ Run value iteration â†’ returns optimal V(s) and greedy policy(s) for GridWorld.

print_value_grid(V,mdp)
â†’ Pretty-print V(s) as a grid (with wall shown).

print_policy_grid(policy,mdp)
â†’ Pretty-print optimal policy as arrows/labels (GOAL, DANG, WALL).





Lab 3 â€“ Functions & Purpose (Super Short Cheatsheet)

init_mrp(gamma=0.5)
â†’ Define MRP: states S, rewards R, transition matrix P, discount gamma.

sample_episode(P,S,start_state_idx=0,terminal_state='sleep',log=True)
â†’ Generate one episode (sequence of state indices) from start until 'sleep'.

compute_return(episode,R,gamma)
â†’ Compute discounted return 
ğº
ğ‘¡
G
t
	â€‹

 for an episode.

debug_return(episode,R,gamma)
â†’ Same as above but prints each stepâ€™s reward, Î³áµ, and partial G_t (for explanation).

mc_value_estimation(S,R,P,gamma,num_episodes,log_interval)
â†’ Monte Carlo estimate of V(s) for all states by averaging returns over many episodes.

bellman_value(P,R,gamma)
â†’ Exact V using Bellman equation: 
ğ‘‰
=
(
ğ¼
âˆ’
ğ›¾
ğ‘ƒ
)
âˆ’
1
ğ‘…
V=(Iâˆ’Î³P)
âˆ’1
R.

change_gamma(old_gamma,new_gamma)
â†’ Helper to â€œchangeâ€ Î³ and log it (for experiments).

change_reward_tv(R,new_reward_tv,state_names)
â†’ Modify reward of 'tv' state in R (what-if reward analysis).

add_state_exercise(S,R,P,gamma,new_transitions_from_exercise,new_rewards_exercise)
â†’ Add new state 'exercise' to MRP, extend S, R, P.





Lab 4 â€“ Functions & Purpose (Ultra Short)

make_frozenlake_env(is_slippery=True,render_mode='ansi')
â†’ Create FrozenLake-v1 env (4Ã—4 or 8Ã—8), choose slippery/deterministic, text render.

value_iteration_frozenlake(env,gamma=0.99,theta=1e-8)
â†’ Run value iteration on FrozenLake â†’ returns optimal V(s).

derive_greedy_policy(env,V,gamma=0.99)
â†’ From a value function V, compute greedy deterministic policy Ï€*(s).

init_random_policy(env)
â†’ Build uniform random policy (each action 1/nA in every state).

policy_evaluation(env,policy,discount_factor=1.0,theta=1e-9,draw=False)
â†’ Policy evaluation (Bellman expectation) â†’ compute V^Ï€ for given policy.

plot_frozenlake(env,V,policy,col_ramp=1,dpi=175,draw_vals=False)
â†’ Draw FrozenLake grid with tiles (S,F,H,G), state values, and policy arrows.





Lab 5 â€“ Functions & Purpose (Super Short)

make_frozenlake_env(is_slippery=True,render_mode='ansi')
â†’ Create FrozenLake-v1 env (slippery/non-slippery).

q_from_v(env,V,s,gamma=1.0)
â†’ Compute Q(s,a) for all actions at a single state s, given V.

policy_improvement(env,V,discount_factor=1.0)
â†’ Greedy policy improvement: from V â†’ deterministic one-hot policy Ï€â€²(s).

plot_frozenlake(V,policy,env,draw_vals=True)
â†’ Show either value grid (numbers) or policy arrows on FrozenLake.

policy_evaluation(env,policy,discount_factor=1.0,theta=1e-9)
â†’ Evaluate a given policy Ï€ â†’ compute V^Ï€ via Bellman expectation.

policy_iteration(env,discount_factor=1.0,theta=1e-9,max_iterations=1000,verbose=False)
â†’ Full policy iteration loop (evaluation + improvement) until policy is stable; returns optimal policy and V.






Lab 6 (Taxi-v3) â€“ Functions & Purpose (Ultra Short)

make_taxi_env(render_mode='ansi')
â†’ Create Taxi-v3 environment.

describe_taxi_env(env)
â†’ Print states, actions, reward structure of Taxi-v3.

value_iteration_taxi(env,discount_factor,theta,max_iterations,return_deltas,verbose)
â†’ Run value iteration â†’ returns optimal V, optimal policy, iterations (and deltas per iter if needed).

extract_policy_from_v_taxi(env,V,discount_factor)
â†’ From a value function V, compute greedy deterministic policy (one-hot per state).

plot_taxi_values(V,title)
â†’ Line plot of V(s) for all 500 states.

plot_taxi_policy(policy,title)
â†’ Bar chart of best action per state (0â€“5).

plot_delta_convergence(deltas,title)
â†’ Plot max |Î”V| per iteration (log scale) to show convergence.

evaluate_policy_taxi(env,policy,n_episodes,discount_factor)
â†’ Run episodes using policy â†’ compute success rate, avg steps per success, avg discounted return.

experiment_gamma_taxi(gammas,theta,max_iterations,n_eval_episodes)
â†’ Run value iteration for multiple Î³ values, compare iterations, success rate, avg steps, avg return, return results dict.






Lab 7 (FrozenLake) â€“ Functions & Purpose (Super Short)

make_frozenlake_env(is_slippery=False,render_mode=None)
â†’ Create FrozenLake-v1 env (usually deterministic for MC/TD comparison).

make_random_policy(env)
â†’ Dict policy: each state â†’ random fixed action (0â€“3).

make_deterministic_policy(env,mode="right")
â†’ Simple fixed policy: always RIGHT / DOWN / etc. for all states.

mc_prediction(env,policy,episodes,gamma)
â†’ First-visit Monte Carlo policy evaluation â†’ returns V, V_track, rewards_per_episode.

td_prediction(env,policy,episodes,alpha,gamma)
â†’ TD(0) policy evaluation (bootstrapping each step) â†’ returns V, V_track, rewards_per_episode.

plot_convergence(V_track,title)
â†’ Plot V(s) vs episodes for all states (shows convergence behavior).

plot_avg_reward(rewards_mc,rewards_td,window)
â†’ Moving-average episode reward for MC vs TD (learning performance).

run_mc_td_experiment(episodes,gamma,alpha,policy_mode,is_slippery)
â†’ Full lab pipeline: build env, choose policy, run MC + TD(0), print Vs, plot convergence & rewards, return all results.